# ğŸš€ Local AI Chat App with Ollama & Streamlit

ğŸ¤– A complete guide to setting up and running your own local ChatGPT-style interface using open-source technology.

## ğŸ“‹ Prerequisites

- Python 3.8+
- Ollama installed and running ([Installation Guide](https://ollama.ai/))
- At least one Ollama model downloaded (e.g., \`ollama pull llama2\`)

## ğŸ› ï¸ Installation

1. Clone this repository
   \`\`\`bash
   git clone https://github.com/yourusername/ollama-streamlit-chat.git
   \`\`\`
   
2. Navigate to project directory
   \`\`\`bash
   cd ollama-streamlit-chat
   \`\`\`
   
3. Install dependencies
   \`\`\`bash
   pip install -r requirements.txt
   \`\`\`

## ğŸš¦ Running the Application

1. Start Ollama in a separate terminal
   \`\`\`bash
   ollama serve
   \`\`\`
   
2. Run the Streamlit app
   \`\`\`bash
   streamlit run app.py
   \`\`\`
   
3. Open browser to \`http://localhost:8501\`

## ğŸ§  Project Structure

\`\`\`
.
â”œâ”€â”€ app.py             # Main application logic
â”œâ”€â”€ requirements.txt   # Python dependencies
â””â”€â”€ README.md          # This documentation
\`\`\`

## ğŸ”§ Customization

- **Add Models**: Use \`ollama pull <model-name>\` to add more models
- **UI Modifications**: Edit \`app.py\` to change:
  - Page title/icon
  - Color scheme
  - Chat interface layout
- **Model Parameters**: Modify temperature/max_tokens in API request

## ğŸš¨ Troubleshooting

**Common Issues:**
- ğŸ”Œ Connection Errors: Ensure Ollama is running in background
- ğŸ“¦ Missing Models: Use \`ollama list\` to verify installed models
- ğŸ Python Dependencies: Verify with \`pip freeze | grep streamlit\`

## ğŸ¤– Available Models

Popular options:
- llama2
- mistral
- codellama
- phi3

Find more at [Ollama Library](https://ollama.ai/library)
