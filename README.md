# ğŸš€ Local AI Chat App with Ollama & Streamlit

ğŸ¤– A complete guide to setting up and running your own local ChatGPT-style interface using open-source technology.

![Project Demo](https://img.shields.io/badge/Demo-Local_AI_Chat-blue) 
![Python Version](https://img.shields.io/badge/Python-3.8%2B-green)
![License](https://img.shields.io/badge/License-MIT-yellow)

## ğŸ“‹ Prerequisites

- Python 3.8+
- [Ollama](https://ollama.ai/) installed and running
- At least one Ollama model downloaded (e.g., `ollama pull llama2`)

## ğŸ› ï¸ Installation

1. Clone the repository:
```bash
   git clone https://github.com/yourusername/ollama-streamlit-chat.git
```
Navigate to project directory:

```bash
   cd ollama-streamlit-chat
```
Install dependencies:

```bash
   pip install -r requirements.txt
```

ğŸš€ Usage
Running the Application
Start Ollama in a separate terminal:

```bash
   ollama serve
```
Launch the Streamlit app:

```bash
   streamlit run app.py
```
Open your browser to http://localhost:8501

ğŸ“‚ Project Structure
```
â”œâ”€â”€ app.py             # Main application logic
â”œâ”€â”€ requirements.txt   # Python dependencies
â””â”€â”€ README.md          # Project documentation
```
ğŸ”§ Customization
Model Management
Add new models: ollama pull <model-name>

List installed models: ollama list

UI Customization
Edit app.py to modify:

Page title/icon (st.set_page_config)

Color scheme (using Streamlit themes)

Chat interface layout

Model Parameters
Modify the API request in app.py to adjust:

temperature

max_tokens

top_p

ğŸš¨ Troubleshooting
Common Issues
ğŸ”Œ Connection Errors: Ensure Ollama is running in background

```bash
   ps aux | grep ollama
```
ğŸ“¦ Missing Models: Verify installed models

```bash
   ollama list
```
ğŸ Python Dependencies: Confirm package versions

```bash
   pip freeze | grep -E 'streamlit|requests'
```
ğŸ¤– Supported Models
|Model Name	|Description|
|-----------|:-----------------------------:|
|llama2	|Meta's versatile LLM|
|mistral	|High-quality English/French model|
|codellama	|Specialized for programming tasks|
|phi3	|Lightweight Microsoft model|

ğŸ“š Explore more models at Ollama Library

ğŸ“„ License
This project is licensed under the MIT License - see the LICENSE file for details.

ğŸ’¡ Pro Tip: Add export OLLAMA_HOST=127.0.0.1:11434 to your .bashrc for persistent Ollama configuration!
